{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer approach (HuggingFace API)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 16:35:19.669623: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-23 16:35:19.825197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737664519.901993 1000629 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737664519.923354 1000629 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-23 16:35:20.094694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from evaluate import load\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "# Text summarization is primarily evaluated through Rouge score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = pd.read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv') # Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "dateAdded                 0\n",
      "dateUpdated               0\n",
      "name                      0\n",
      "asins                     0\n",
      "brand                     0\n",
      "categories                0\n",
      "primaryCategories         0\n",
      "imageURLs                 0\n",
      "keys                      0\n",
      "manufacturer              0\n",
      "manufacturerNumber        0\n",
      "reviews.date              0\n",
      "reviews.dateAdded      3948\n",
      "reviews.dateSeen          0\n",
      "reviews.doRecommend       0\n",
      "reviews.id             4971\n",
      "reviews.numHelpful        0\n",
      "reviews.rating            0\n",
      "reviews.sourceURLs        0\n",
      "reviews.text              0\n",
      "reviews.title            13\n",
      "reviews.username          1\n",
      "sourceURLs                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_df.isnull().sum()) # No NULLs found in categories, reviews.text or reviews.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate dataframe into product categories, reviews and ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df_categories = kaggle_df['categories'] # Categories\n",
    "kaggle_df_reviews = kaggle_df['reviews.text'] # Reviews\n",
    "kaggle_df_ratings = kaggle_df['reviews.rating'] # Rating\n",
    "\n",
    "all_categories = list(kaggle_df['categories'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>I was looking for a kindle whitepaper. I saw o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>Looking at the picture and seeing it was 8th g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Computers,Amazon Echo,Virtual Assistant Speake...</td>\n",
       "      <td>Purchased this device at launch (2 pack for $3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Computers,Amazon Echo,Virtual Assistant Speake...</td>\n",
       "      <td>I waited a couple months to review giving Amaz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Computers,Amazon Echo,Virtual Assistant Speake...</td>\n",
       "      <td>qc is really bad on this product and does not ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>Tablets,Fire Tablets,Computers &amp; Tablets,All T...</td>\n",
       "      <td>The last 2 models of Kindle HDX 8 have been te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>Kindle E-readers,Electronics Features,Computer...</td>\n",
       "      <td>This is not an upgrade by any means! My three ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4823</th>\n",
       "      <td>Fire Tablets,Tablets,Computers/Tablets &amp; Netwo...</td>\n",
       "      <td>Bought this mostly as a backup.and to read a f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>Fire Tablets,Tablets,Computers/Tablets &amp; Netwo...</td>\n",
       "      <td>The last 2 models of Kindle HDX 8 have been te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>Tablets,Fire Tablets,Electronics,iPad &amp; Tablet...</td>\n",
       "      <td>Very cheap and was not impressed at all never ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             categories  \\\n",
       "20    Computers,Electronics Features,Tablets,Electro...   \n",
       "70    Computers,Electronics Features,Tablets,Electro...   \n",
       "265   Computers,Amazon Echo,Virtual Assistant Speake...   \n",
       "361   Computers,Amazon Echo,Virtual Assistant Speake...   \n",
       "504   Computers,Amazon Echo,Virtual Assistant Speake...   \n",
       "...                                                 ...   \n",
       "4761  Tablets,Fire Tablets,Computers & Tablets,All T...   \n",
       "4795  Kindle E-readers,Electronics Features,Computer...   \n",
       "4823  Fire Tablets,Tablets,Computers/Tablets & Netwo...   \n",
       "4865  Fire Tablets,Tablets,Computers/Tablets & Netwo...   \n",
       "4953  Tablets,Fire Tablets,Electronics,iPad & Tablet...   \n",
       "\n",
       "                                           reviews.text  reviews.rating  \n",
       "20    I was looking for a kindle whitepaper. I saw o...               1  \n",
       "70    Looking at the picture and seeing it was 8th g...               1  \n",
       "265   Purchased this device at launch (2 pack for $3...               1  \n",
       "361   I waited a couple months to review giving Amaz...               1  \n",
       "504   qc is really bad on this product and does not ...               1  \n",
       "...                                                 ...             ...  \n",
       "4761  The last 2 models of Kindle HDX 8 have been te...               1  \n",
       "4795  This is not an upgrade by any means! My three ...               1  \n",
       "4823  Bought this mostly as a backup.and to read a f...               1  \n",
       "4865  The last 2 models of Kindle HDX 8 have been te...               1  \n",
       "4953  Very cheap and was not impressed at all never ...               1  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "star1_summaries = kaggle_df[kaggle_df['reviews.rating'] == 1]\n",
    "star1_summaries = star1_summaries[['categories', 'reviews.text', 'reviews.rating']]\n",
    "\n",
    "star2_summaries = kaggle_df[kaggle_df['reviews.rating'] == 2]\n",
    "star2_summaries = star2_summaries[['categories', 'reviews.text', 'reviews.rating']]\n",
    "\n",
    "star3_summaries = kaggle_df[kaggle_df['reviews.rating'] == 3]\n",
    "star3_summaries = star3_summaries[['categories', 'reviews.text', 'reviews.rating']]\n",
    "\n",
    "star4_summaries = kaggle_df[kaggle_df['reviews.rating'] == 4]\n",
    "star4_summaries = star4_summaries[['categories', 'reviews.text', 'reviews.rating']]\n",
    "\n",
    "star5_summaries = kaggle_df[kaggle_df['reviews.rating'] == 5]\n",
    "star5_summaries = star5_summaries[['categories', 'reviews.text', 'reviews.rating']]\n",
    "\n",
    "display(star1_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize the list into one summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the list into one, function\n",
    "\n",
    "def summaries_into_one(dataframe):\n",
    "    # Initialize the model and tokenizer\n",
    "    model_name = \"t5-small\" # model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name, device_map={\"\": 0})\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Join reviews into a single string\n",
    "    text = \"\\n\\n\".join(dataframe)\n",
    "\n",
    "    # Tokenize and summarize the input text. inputs is a pytorch tensor, torch.Tensor\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors = \"pt\", truncation = True).to(\"cuda:0\") # max_length = 2048 excluded\n",
    "\n",
    "    # summary_ids is a pytorch tensor, torch.Tensor\n",
    "    summary_ids = model.generate(inputs, max_length = 250, min_length = 50, length_penalty = 2.0, num_beams = 4, early_stopping = True)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "\n",
    "    return(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary of {categories: list of summaries of reviews by stars} and ROUGE evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.0392156862745098, 'rouge2': 0.0, 'rougeL': 0.0392156862745098, 'rougeLsum': 0.0392156862745098}\n",
      "{'rouge1': 0.046511627906976744, 'rouge2': 0.0, 'rougeL': 0.046511627906976744, 'rougeLsum': 0.046511627906976744}\n",
      "{'rouge1': 0.04938271604938271, 'rouge2': 0.0, 'rougeL': 0.04938271604938271, 'rougeLsum': 0.04938271604938271}\n",
      "{'rouge1': 0.2184873949579832, 'rouge2': 0.0, 'rougeL': 0.2184873949579832, 'rougeLsum': 0.2184873949579832}\n",
      "{'rouge1': 0.038461538461538464, 'rouge2': 0.0, 'rougeL': 0.038461538461538464, 'rougeLsum': 0.038461538461538464}\n",
      "{'rouge1': 0.05533596837944664, 'rouge2': 0.0, 'rougeL': 0.05533596837944664, 'rougeLsum': 0.05533596837944664}\n",
      "{'rouge1': 0.05555555555555555, 'rouge2': 0.0, 'rougeL': 0.05555555555555555, 'rougeLsum': 0.05555555555555555}\n",
      "{'rouge1': 0.026415094339622643, 'rouge2': 0.0, 'rougeL': 0.026415094339622643, 'rougeLsum': 0.026415094339622643}\n",
      "{'rouge1': 0.06349206349206349, 'rouge2': 0.0, 'rougeL': 0.06349206349206349, 'rougeLsum': 0.06349206349206349}\n",
      "{'rouge1': 0.04040404040404041, 'rouge2': 0.0, 'rougeL': 0.04040404040404041, 'rougeLsum': 0.04040404040404041}\n",
      "{'rouge1': 0.048327137546468404, 'rouge2': 0.0, 'rougeL': 0.048327137546468404, 'rougeLsum': 0.048327137546468404}\n",
      "{'rouge1': 0.06048387096774194, 'rouge2': 0.0, 'rougeL': 0.06048387096774194, 'rougeLsum': 0.06048387096774194}\n",
      "{'rouge1': 0.04229607250755287, 'rouge2': 0.0, 'rougeL': 0.04229607250755287, 'rougeLsum': 0.04229607250755287}\n",
      "{'rouge1': 0.0421455938697318, 'rouge2': 0.0, 'rougeL': 0.0421455938697318, 'rougeLsum': 0.0421455938697318}\n",
      "{'rouge1': 0.27672955974842767, 'rouge2': 0.0, 'rougeL': 0.27672955974842767, 'rougeLsum': 0.27672955974842767}\n",
      "{'rouge1': 0.034482758620689655, 'rouge2': 0.0, 'rougeL': 0.034482758620689655, 'rougeLsum': 0.034482758620689655}\n",
      "{'rouge1': 0.046610169491525424, 'rouge2': 0.0, 'rougeL': 0.046610169491525424, 'rougeLsum': 0.046610169491525424}\n",
      "{'rouge1': 0.03076923076923077, 'rouge2': 0.0, 'rougeL': 0.03076923076923077, 'rougeLsum': 0.03076923076923077}\n",
      "{'rouge1': 0.0321285140562249, 'rouge2': 0.0, 'rougeL': 0.0321285140562249, 'rougeLsum': 0.0321285140562249}\n",
      "{'rouge1': 0.784688995215311, 'rouge2': 0.0, 'rougeL': 0.784688995215311, 'rougeLsum': 0.784688995215311}\n",
      "{'rouge1': 0.02631578947368421, 'rouge2': 0.0, 'rougeL': 0.02631578947368421, 'rougeLsum': 0.02631578947368421}\n",
      "{'rouge1': 0.033962264150943396, 'rouge2': 0.0, 'rougeL': 0.033962264150943396, 'rougeLsum': 0.033962264150943396}\n",
      "{'rouge1': 0.047872340425531915, 'rouge2': 0.0, 'rougeL': 0.047872340425531915, 'rougeLsum': 0.047872340425531915}\n",
      "{'rouge1': 0.04054054054054054, 'rouge2': 0.0, 'rougeL': 0.04054054054054054, 'rougeLsum': 0.04054054054054054}\n",
      "{'rouge1': 0.21223021582733814, 'rouge2': 0.0, 'rougeL': 0.21223021582733814, 'rougeLsum': 0.21223021582733814}\n",
      "{'rouge1': 0.022222222222222223, 'rouge2': 0.0, 'rougeL': 0.022222222222222223, 'rougeLsum': 0.022222222222222223}\n",
      "{'rouge1': 0.036585365853658534, 'rouge2': 0.0, 'rougeL': 0.036585365853658534, 'rougeLsum': 0.036585365853658534}\n",
      "{'rouge1': 0.021645021645021644, 'rouge2': 0.0, 'rougeL': 0.021645021645021644, 'rougeLsum': 0.021645021645021644}\n",
      "{'rouge1': 0.06302521008403361, 'rouge2': 0.0, 'rougeL': 0.06302521008403361, 'rougeLsum': 0.06302521008403361}\n",
      "{'rouge1': 0.43037974683544306, 'rouge2': 0.0, 'rougeL': 0.43037974683544306, 'rougeLsum': 0.43037974683544306}\n",
      "{'rouge1': 0.046052631578947366, 'rouge2': 0.0, 'rougeL': 0.046052631578947366, 'rougeLsum': 0.046052631578947366}\n",
      "{'rouge1': 0.03208556149732621, 'rouge2': 0.0, 'rougeL': 0.03208556149732621, 'rougeLsum': 0.03208556149732621}\n",
      "{'rouge1': 0.032362459546925564, 'rouge2': 0.0, 'rougeL': 0.032362459546925564, 'rougeLsum': 0.032362459546925564}\n",
      "{'rouge1': 0.03888888888888889, 'rouge2': 0.0, 'rougeL': 0.03888888888888889, 'rougeLsum': 0.03888888888888889}\n",
      "{'rouge1': 0.0398406374501992, 'rouge2': 0.0, 'rougeL': 0.0398406374501992, 'rougeLsum': 0.0398406374501992}\n",
      "{'rouge1': 0.06923076923076923, 'rouge2': 0.0, 'rougeL': 0.06923076923076923, 'rougeLsum': 0.06923076923076923}\n",
      "{'rouge1': 0.06415094339622641, 'rouge2': 0.0, 'rougeL': 0.06415094339622641, 'rougeLsum': 0.06415094339622641}\n",
      "{'rouge1': 0.041379310344827586, 'rouge2': 0.0, 'rougeL': 0.041379310344827586, 'rougeLsum': 0.041379310344827586}\n",
      "{'rouge1': 0.02631578947368421, 'rouge2': 0.0, 'rougeL': 0.02631578947368421, 'rougeLsum': 0.02631578947368421}\n",
      "{'rouge1': 0.2679425837320574, 'rouge2': 0.0, 'rougeL': 0.2679425837320574, 'rougeLsum': 0.2679425837320574}\n",
      "{'rouge1': 0.046012269938650305, 'rouge2': 0.0, 'rougeL': 0.046012269938650305, 'rougeLsum': 0.046012269938650305}\n",
      "{'rouge1': 0.038461538461538464, 'rouge2': 0.0, 'rougeL': 0.038461538461538464, 'rougeLsum': 0.038461538461538464}\n",
      "{'rouge1': 0.29880478087649404, 'rouge2': 0.0, 'rougeL': 0.29880478087649404, 'rougeLsum': 0.2948207171314741}\n",
      "{'rouge1': 0.03286384976525822, 'rouge2': 0.0, 'rougeL': 0.03286384976525822, 'rougeLsum': 0.03286384976525822}\n",
      "{'rouge1': 0.030434782608695653, 'rouge2': 0.0, 'rougeL': 0.030434782608695653, 'rougeLsum': 0.030434782608695653}\n",
      "{'rouge1': 0.04504504504504504, 'rouge2': 0.0, 'rougeL': 0.04504504504504504, 'rougeLsum': 0.04504504504504504}\n",
      "{'rouge1': 0.004784688995215311, 'rouge2': 0.0, 'rougeL': 0.004784688995215311, 'rougeLsum': 0.004784688995215311}\n",
      "{'rouge1': 0.03430079155672823, 'rouge2': 0.0, 'rougeL': 0.03430079155672823, 'rougeLsum': 0.03430079155672823}\n",
      "{'rouge1': 0.03571428571428571, 'rouge2': 0.0, 'rougeL': 0.03571428571428571, 'rougeLsum': 0.03571428571428571}\n",
      "{'rouge1': 0.053811659192825115, 'rouge2': 0.0, 'rougeL': 0.053811659192825115, 'rougeLsum': 0.053811659192825115}\n",
      "{'rouge1': 0.04524886877828054, 'rouge2': 0.0, 'rougeL': 0.04524886877828054, 'rougeLsum': 0.04524886877828054}\n",
      "{'rouge1': 0.04395604395604396, 'rouge2': 0.0, 'rougeL': 0.04395604395604396, 'rougeLsum': 0.04395604395604396}\n",
      "{'rouge1': 0.04964539007092199, 'rouge2': 0.0, 'rougeL': 0.04964539007092199, 'rougeLsum': 0.04964539007092199}\n",
      "{'rouge1': 0.030042918454935622, 'rouge2': 0.0, 'rougeL': 0.030042918454935622, 'rougeLsum': 0.030042918454935622}\n",
      "{'rouge1': 0.6833976833976834, 'rouge2': 0.0, 'rougeL': 0.6833976833976834, 'rougeLsum': 0.6833976833976834}\n",
      "{'rouge1': 0.03076923076923077, 'rouge2': 0.0, 'rougeL': 0.03076923076923077, 'rougeLsum': 0.03076923076923077}\n",
      "{'rouge1': 0.03211009174311927, 'rouge2': 0.0, 'rougeL': 0.03211009174311927, 'rougeLsum': 0.03211009174311927}\n",
      "{'rouge1': 0.037037037037037035, 'rouge2': 0.0, 'rougeL': 0.037037037037037035, 'rougeLsum': 0.037037037037037035}\n",
      "{'rouge1': 0.06132075471698113, 'rouge2': 0.0, 'rougeL': 0.06132075471698113, 'rougeLsum': 0.06132075471698113}\n",
      "{'rouge1': 0.21052631578947367, 'rouge2': 0.0, 'rougeL': 0.21052631578947367, 'rougeLsum': 0.21052631578947367}\n",
      "{'rouge1': 0.04326923076923077, 'rouge2': 0.0, 'rougeL': 0.04326923076923077, 'rougeLsum': 0.04326923076923077}\n",
      "{'rouge1': 0.528169014084507, 'rouge2': 0.0, 'rougeL': 0.528169014084507, 'rougeLsum': 0.528169014084507}\n",
      "{'rouge1': 0.030456852791878174, 'rouge2': 0.0, 'rougeL': 0.030456852791878174, 'rougeLsum': 0.030456852791878174}\n",
      "{'rouge1': 0.03773584905660377, 'rouge2': 0.0, 'rougeL': 0.03773584905660377, 'rougeLsum': 0.03773584905660377}\n",
      "{'rouge1': 0.04905660377358491, 'rouge2': 0.0, 'rougeL': 0.04905660377358491, 'rougeLsum': 0.04905660377358491}\n",
      "{'rouge1': 0.015544041450777202, 'rouge2': 0.0, 'rougeL': 0.015544041450777202, 'rougeLsum': 0.015544041450777202}\n",
      "{'rouge1': 0.05, 'rouge2': 0.0, 'rougeL': 0.05, 'rougeLsum': 0.05}\n",
      "{'rouge1': 0.06829268292682927, 'rouge2': 0.0, 'rougeL': 0.06829268292682927, 'rougeLsum': 0.06829268292682927}\n",
      "{'rouge1': 0.04365079365079365, 'rouge2': 0.0, 'rougeL': 0.04365079365079365, 'rougeLsum': 0.04365079365079365}\n",
      "{'rouge1': 0.028112449799196786, 'rouge2': 0.0, 'rougeL': 0.028112449799196786, 'rougeLsum': 0.028112449799196786}\n",
      "{'rouge1': 0.03349282296650718, 'rouge2': 0.0, 'rougeL': 0.03349282296650718, 'rougeLsum': 0.03349282296650718}\n",
      "{'rouge1': 0.04326923076923077, 'rouge2': 0.0, 'rougeL': 0.04326923076923077, 'rougeLsum': 0.04326923076923077}\n",
      "{'rouge1': 0.033707865168539325, 'rouge2': 0.0, 'rougeL': 0.033707865168539325, 'rougeLsum': 0.033707865168539325}\n",
      "{'rouge1': 0.04421768707482993, 'rouge2': 0.0, 'rougeL': 0.04421768707482993, 'rougeLsum': 0.04421768707482993}\n",
      "{'rouge1': 0.04395604395604396, 'rouge2': 0.0, 'rougeL': 0.04395604395604396, 'rougeLsum': 0.04395604395604396}\n",
      "{'rouge1': 0.009478672985781991, 'rouge2': 0.0, 'rougeL': 0.009478672985781991, 'rougeLsum': 0.009478672985781991}\n",
      "{'rouge1': 0.03333333333333333, 'rouge2': 0.0, 'rougeL': 0.03333333333333333, 'rougeLsum': 0.03333333333333333}\n",
      "{'rouge1': 0.03349282296650718, 'rouge2': 0.0, 'rougeL': 0.03349282296650718, 'rougeLsum': 0.03349282296650718}\n",
      "{'rouge1': 0.07894736842105263, 'rouge2': 0.0, 'rougeL': 0.07894736842105263, 'rougeLsum': 0.07894736842105263}\n",
      "{'rouge1': 0.03529411764705882, 'rouge2': 0.0, 'rougeL': 0.03529411764705882, 'rougeLsum': 0.03529411764705882}\n",
      "{'rouge1': 0.047058823529411764, 'rouge2': 0.0, 'rougeL': 0.047058823529411764, 'rougeLsum': 0.047058823529411764}\n",
      "{'rouge1': 0.009569377990430622, 'rouge2': 0.0, 'rougeL': 0.009569377990430622, 'rougeLsum': 0.009569377990430622}\n",
      "{'rouge1': 0.16751269035532995, 'rouge2': 0.0, 'rougeL': 0.16751269035532995, 'rougeLsum': 0.16751269035532995}\n",
      "{'rouge1': 0.4435146443514644, 'rouge2': 0.0, 'rougeL': 0.4435146443514644, 'rougeLsum': 0.4435146443514644}\n",
      "{'rouge1': 0.05319148936170213, 'rouge2': 0.0, 'rougeL': 0.05319148936170213, 'rougeLsum': 0.05319148936170213}\n",
      "{'rouge1': 0.0390625, 'rouge2': 0.0, 'rougeL': 0.0390625, 'rougeLsum': 0.0390625}\n",
      "rouge1 average: 0.0873561269402484 - rouge2 average: 0.0 - rougeL average: 0.0873561269402484 - rougeLsum average: 0.0873098006176319\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluation variable\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Counters to calculate average\n",
    "sumRouge1 = 0\n",
    "sumRouge2 = 0\n",
    "sumRougeL = 0\n",
    "sumRougeLSum = 0\n",
    "\n",
    "sum_len = 0\n",
    "\n",
    "# Each category has a list of 5 summaries of all reviews, 1 summary per star\n",
    "category_dict = {}\n",
    "\n",
    "# Build a dictionary \n",
    "for key in all_categories:\n",
    "    category_dict.setdefault(key, [])\n",
    "\n",
    "grouped = kaggle_df.groupby(['categories', 'reviews.rating'])\n",
    "\n",
    "# Assign the summaries to the lists\n",
    "for category in all_categories: # for each product category \n",
    "    for rating in range(1, 6): # from 1 stars to 5 stars, rating\n",
    "        try:\n",
    "            all_reviews_same_rating = grouped.get_group((category, rating))['reviews.text']\n",
    "            summ1 = summaries_into_one( all_reviews_same_rating )\n",
    "            category_dict[category].append( summ1 )\n",
    "\n",
    "            results = rouge.compute(predictions = \"\\n\\n\".join(all_reviews_same_rating)[:len(summ1)], references = summ1) # len(summ1) = 247 to avoid mismatch\n",
    "            print(results)\n",
    "\n",
    "            sumRouge1 += results['rouge1']\n",
    "            sumRouge2 += results['rouge2']\n",
    "            sumRougeL += results['rougeL']\n",
    "            sumRougeLSum += results['rougeLsum']\n",
    "\n",
    "            sum_len += 1\n",
    "\n",
    "        except:\n",
    "            category_dict[category].append('NULL')\n",
    "\n",
    "print(\"rouge1 average:\", sumRouge1 / sum_len , \"- rouge2 average:\", sumRouge2 / sum_len , \"- rougeL average:\", sumRougeL / sum_len ,\"- rougeLsum average:\", sumRougeLSum / sum_len)\n",
    "\n",
    "# rouge1 average: 0.0873561269402484 - rouge2 average: 0.0 - rougeL average: 0.08728537224163033 - rougeLsum average: 0.0873561269402484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Summary of reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>1</td>\n",
       "      <td>the whitepaper looks Identical to the $120 mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>2</td>\n",
       "      <td>screen too dark The screen is too dark, and ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>3</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>4</td>\n",
       "      <td>the kindle is good to download apps for books ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Computers,Electronics Features,Tablets,Electro...</td>\n",
       "      <td>5</td>\n",
       "      <td>the amazon Kindle is light weight and easy to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Tablets,Fire Tablets,Electronics,iPad &amp; Tablet...</td>\n",
       "      <td>1</td>\n",
       "      <td>very cheap and was not impressed at all never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Tablets,Fire Tablets,Electronics,iPad &amp; Tablet...</td>\n",
       "      <td>2</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Tablets,Fire Tablets,Electronics,iPad &amp; Tablet...</td>\n",
       "      <td>3</td>\n",
       "      <td>the battery is having more and more trouble ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Tablets,Fire Tablets,Electronics,iPad &amp; Tablet...</td>\n",
       "      <td>4</td>\n",
       "      <td>my daughter has had this tablet for almost 2 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Tablets,Fire Tablets,Electronics,iPad &amp; Tablet...</td>\n",
       "      <td>5</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Product Category  Rating  \\\n",
       "0    Computers,Electronics Features,Tablets,Electro...       1   \n",
       "1    Computers,Electronics Features,Tablets,Electro...       2   \n",
       "2    Computers,Electronics Features,Tablets,Electro...       3   \n",
       "3    Computers,Electronics Features,Tablets,Electro...       4   \n",
       "4    Computers,Electronics Features,Tablets,Electro...       5   \n",
       "..                                                 ...     ...   \n",
       "110  Tablets,Fire Tablets,Electronics,iPad & Tablet...       1   \n",
       "111  Tablets,Fire Tablets,Electronics,iPad & Tablet...       2   \n",
       "112  Tablets,Fire Tablets,Electronics,iPad & Tablet...       3   \n",
       "113  Tablets,Fire Tablets,Electronics,iPad & Tablet...       4   \n",
       "114  Tablets,Fire Tablets,Electronics,iPad & Tablet...       5   \n",
       "\n",
       "                                    Summary of reviews  \n",
       "0    the whitepaper looks Identical to the $120 mod...  \n",
       "1    screen too dark The screen is too dark, and ca...  \n",
       "2                                                 NULL  \n",
       "3    the kindle is good to download apps for books ...  \n",
       "4    the amazon Kindle is light weight and easy to ...  \n",
       "..                                                 ...  \n",
       "110  very cheap and was not impressed at all never ...  \n",
       "111                                               NULL  \n",
       "112  the battery is having more and more trouble ho...  \n",
       "113  my daughter has had this tablet for almost 2 m...  \n",
       "114                                               NULL  \n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_dict_df = pd.DataFrame(columns = ['Product Category', 'Rating', 'Summary of reviews'])\n",
    "\n",
    "for key in category_dict: # for each product category \n",
    "    for rating in range(1, 6): # from 1 stars to 5 stars, rating\n",
    "        category_dict_df.loc[len(category_dict_df)] = [key, rating, category_dict[key][rating - 1]]\n",
    "\n",
    "display(category_dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.7833333333333332, 'rouge2': 0.5833333333333334, 'rougeL': 0.7833333333333332, 'rougeLsum': 0.7833333333333332}\n",
      "0.7833333333333332\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Evaluation example ---------------------------\n",
    "rouge = evaluate.load('rouge')\n",
    "candidates = [\"Summarization is cool\",\"I love Machine Learning\",\"Good night\"]\n",
    "\n",
    "references = [\n",
    "[\"Summarization is beneficial and cool\",\"Summarization saves time\"],\n",
    "[\"People are getting used to Machine Learning\",\"I think i love Machine Learning\"],\n",
    "[\"Good night everyone!\",\"Night!\"]\n",
    "             ]\n",
    "\n",
    "results = rouge.compute(predictions = candidates, references = references)\n",
    "print(results)\n",
    "\n",
    "print(results['rouge1'])\n",
    "# -------------------------- Evaluation example ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    I was looking for a kindle whitepaper. I saw o...\n",
      "70    Looking at the picture and seeing it was 8th g...\n",
      "Name: reviews.text, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      "Dataset({\n",
      "    features: ['File_path', 'Articles', 'Summaries'],\n",
      "    num_rows: 1779\n",
      "})\n",
      "Dataset({\n",
      "    features: ['File_path', 'Articles', 'Summaries'],\n",
      "    num_rows: 445\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('gopalkalpande/bbc-news-summary', split = 'train')\n",
    "full_dataset = dataset.train_test_split(test_size = 0.2, shuffle = True)\n",
    "\n",
    "text = grouped.get_group((all_categories[0], 1))['reviews.text'] # text = grouped.get_group((all_categories[0], 5))['reviews.text']\n",
    "#text = \"\\n\\n\".join(text)\n",
    "print(text)\n",
    "print(type(text))\n",
    "\n",
    "'''dataset_train = Dataset.from_pandas(kaggle_df)\n",
    "dataset_valid = Dataset.from_pandas(category_dict_df)'''\n",
    "\n",
    "dataset_train = full_dataset['train'] # text? # full_dataset['train'] \n",
    "dataset_valid = full_dataset['test'] # cambiar por category_dict[all_categories[0]][0]? # full_dataset['test']\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_valid)\n",
    "\n",
    "print(type(dataset_train))\n",
    "print(type(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 't5-small'\n",
    "BATCH_SIZE = 4\n",
    "NUM_PROCS = 4\n",
    "EPOCHS = 10\n",
    "OUT_DIR = 'results_t5small'\n",
    "MAX_LENGTH = 512 # Maximum context length to consider while preparing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86c69af32dd4621934dc04a2d3bc17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7970a81fa1ff429b8dcbba47a8e1d4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
    " \n",
    "# Function to convert text data into model inputs and targets\n",
    "def preprocess_function(examples):\n",
    "    \n",
    "    inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length = MAX_LENGTH,\n",
    "        truncation = True,\n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    targets = [summary for summary in examples['Summaries']]\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length = MAX_LENGTH,\n",
    "            truncation = True,\n",
    "            padding = 'max_length'\n",
    "        )\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "#-------------------------------------\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched = True,\n",
    "    num_proc = NUM_PROCS\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "    batched = True,\n",
    "    num_proc = NUM_PROCS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60,506,624 total parameters.\n",
      "60,506,624 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rouge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    " \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    " \n",
    "    result = rouge.compute(\n",
    "        predictions = decoded_preds,\n",
    "        references = decoded_labels,\n",
    "        use_stemmer = True,\n",
    "        rouge_types = [\n",
    "            'rouge1',\n",
    "            'rouge2',\n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    " \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    " \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim = -1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lexo/tf-gpu/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4450' max='4450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4450/4450 24:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.360115</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.835800</td>\n",
       "      <td>0.887700</td>\n",
       "      <td>225.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345974</td>\n",
       "      <td>0.906300</td>\n",
       "      <td>0.838300</td>\n",
       "      <td>0.890800</td>\n",
       "      <td>225.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.005700</td>\n",
       "      <td>0.329870</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>0.844500</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.005700</td>\n",
       "      <td>0.331684</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.895600</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.326174</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>0.896800</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.315940</td>\n",
       "      <td>0.913400</td>\n",
       "      <td>0.850300</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.319488</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>0.851300</td>\n",
       "      <td>0.899700</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.311932</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.315086</td>\n",
       "      <td>0.915600</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.311092</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>0.852700</td>\n",
       "      <td>0.900300</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.310338</td>\n",
       "      <td>0.916100</td>\n",
       "      <td>0.855100</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.314291</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.313721</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.319559</td>\n",
       "      <td>0.918100</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>0.903100</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.314559</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.319773</td>\n",
       "      <td>0.918500</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.918500</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.326933</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.860600</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.321442</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>0.860400</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>0.919700</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.323951</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.861600</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.324519</td>\n",
       "      <td>0.919800</td>\n",
       "      <td>0.861400</td>\n",
       "      <td>0.905300</td>\n",
       "      <td>225.806700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = OUT_DIR,\n",
    "    num_train_epochs = EPOCHS, # number of epochs\n",
    "\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "\n",
    "    warmup_steps = 500,\n",
    "    \n",
    "    weight_decay = 0.01,\n",
    "\n",
    "    evaluation_strategy = 'steps', # how often will evaluation be during training, each 200 steps\n",
    "    eval_steps = 200,\n",
    "\n",
    "    save_strategy = 'epoch', # how often will saving be during training, each 2 epochs\n",
    "    save_total_limit = 2,\n",
    "\n",
    "    learning_rate = 0.001,\n",
    "    # dataloader_num_workers = 4 # Number of subprocesses to use for data loading\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    \n",
    "    args = training_args,\n",
    "\n",
    "    train_dataset = tokenized_train,\n",
    "    eval_dataset = tokenized_valid,\n",
    "\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics, # This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    compute_metrics = compute_metrics # The function that will be used to compute metrics at evaluation\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loss: 0.200300** \n",
    "**Validation Loss: 0.392833**\t\n",
    "**Rouge1: 0.910000\tRouge2: 0.847000**\n",
    "**Rougel:0.893600**\n",
    "**Gen Len: 233.831500**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{OUT_DIR}/checkpoint-4450\"  # the path where you saved your model\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4450, training_loss=0.36820893191219717, metrics={'train_runtime': 1494.1962, 'train_samples_per_second': 11.906, 'train_steps_per_second': 2.978, 'total_flos': 2407730648186880.0, 'train_loss': 0.36820893191219717, 'epoch': 10.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "transformers.trainer_utils.TrainOutput"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(history)\n",
    "display(type(history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
